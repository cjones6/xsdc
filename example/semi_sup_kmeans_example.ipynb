{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Baseline experiments with semi-supervised k-means </center>\n",
    "\n",
    "The baseline algorithm for unsupervised learning is k-means.\n",
    "When considering data with labeled and unlabeled data, k-means can be modified\n",
    "by using seeds leading to Seeded K-means presented by [1]. In this notebook, we present baseline experiments for semi-supervised learning using seeded k-means as presented in [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised k-means\n",
    "We present below the pesudo-code of the implementation\n",
    "of semi-supervised k-means as done by [1].\n",
    "\n",
    "<center><img src=\"img/seeded_kmeans.png\" alt=\"Semi-Supervised k-means\" width=\"600\" /></center>\n",
    "\n",
    "The algorithm relies on the computation\n",
    "of the distance matrix from the points to the centers of the clusters,\n",
    "i.e., computing $D = (D_{ik})_{\\substack{i=1,\\ldots, n\\\\ k =1, \\ldots K}}$\n",
    "with $D_{ik} = \\|x_i - c_k\\|_2$.\n",
    "Denote then\n",
    "- the cluster centers by $C = (c_1, \\ldots, c_K)^\\top \\in \\mathbb{R}^{K \\times d}$,\n",
    "- the assignment matrix of the points to the given clusters,\n",
    "by $Z \\in \\{0,1\\}^{n \\times K}$ such that $Z_{ik}=1$ if and only if\n",
    "point $i$ belongs to cluster $k$\n",
    "- the matrix of points by $X = (x_1, \\ldots, x_n)^\\top \\in \\mathbb{R}^{n \\times d}$.\n",
    "\n",
    "The matrix D is given by\n",
    "$$ D = \\textrm{diag}(X X^\\top)1_K^\\top - 2XC^\\top + 1_n \\textrm{diag}(C C^\\top)^\\top $$\n",
    "where $1_n$ denotes the vector of size $n$ composed of ones.\n",
    "\n",
    "For a fixed assignment matrix $Z$, the centers are given\n",
    "as $C = (Z^\\top Z)^{-1} Z^\\top X$; hence the matrix of distances\n",
    "can be computed directly from the knowledge of the Gram matrix\n",
    "$G= X^\\top X$ and the assignment matrix $Z$.\n",
    "\n",
    "As a consequence, we can adapt the implementation of the algorithm of [1]\n",
    "to use a Gram matrix defined by any kernel. The overall algorithm is presented below.\n",
    "We use $\\delta_{ik}$ to denote the Kronecker delta symbol such that\n",
    "$\\delta_{jk} = 1$ if $j =k$ and 0  otherwise.\n",
    "\n",
    "<center><img src=\"img/kernel_seeded_kmeans.png\" alt=\"Semi-Supervised k-means with generic Gram matrix\" width=\"600\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel choices\n",
    "For the rest of the implementation, the main computational bottleneck\n",
    "is the computation of the Gram matrix $G$, which we will parameterize using a kernel, namely, $G = (h(x_i, x_j))_{i,\n",
    "j=1}^n$. The kernels we consider are\n",
    "-  a linear kernel: $h(x, y) = x^\\top y$,\n",
    "which amounts to clustering points with respect to the squared Euclidean distances in the original feature space;\n",
    "- a data-dependent similarity measure defined by the regularized inverse correlation matrix:\n",
    "$h(x, y) = x^\\top (X^\\top X/(n-1) +\\lambda I)^{-1} y$,\n",
    "where $X= (x_1, \\ldots, x_n)^\\top \\in \\mathbb{R}^{n \\times d}$\n",
    "is the set of all standardized training points and $\\lambda \\geq 0$ is a regularization parameter;\n",
    "- a non-linear similarity measure defined by a Gaussian Radial Basis Function (RBF) kernel:\n",
    "$h(x, y) = \\exp\\left(- \\|x-y\\|_2^2/(2\\sigma^2)\\right)$ for some bandwidth parameter $\\sigma>0$,\n",
    "which amounts to clustering points in the reproducing kernel Hilbert space associated with $h$.\n",
    "\n",
    "The bandwidth parameter of the RBF kernel is chosen using the median heuristic.\n",
    "Namely, we choose $\\sigma = \\sqrt{2 d}$, where $d = \\textrm{Median}((\\|x_i-x_j\\|_2^2)_{1\\leq i<j\\leq n})$\n",
    "is the median squared distance computed from all data (unlabeled and labeled ones) [4].\n",
    "The regularization parameter for the similarity measure based on the correlation matrix is chosen using the\n",
    "heuristic $\\lambda = \\lambda_{\\max}(C)/\\mathrm{Tr}(C)$, where $C = X^\\top X/(n-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data preprocessing\n",
    "The data is selected from one of the datasets 'gisette', 'magic' or 'mnist'. Instructions on how to download them may be found in the\n",
    "README file. \n",
    "The data is standardized (based on the ``scaling`` parameter in the following); see the code in the file ``get_data``.\n",
    "Then the reference hyper-parameters, i.e., the bandwidth of the Gaussian kernel or the regularization\n",
    "of the data-dependent kernel, are computed from the data (function ``get_hyper_params_ref``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import scipy.sparse.linalg as splinalg\n",
    "import sys\n",
    "\n",
    "sys.path.append('../experiments')\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "from baselines.kernels_utils import RBF\n",
    "from baselines.get_data import keep_labels\n",
    "\n",
    "from baselines.pipeline_utils import var_to_str\n",
    "# The function var_to_str is a simple helper that transforms e.g.,\n",
    "# a dictionary into a string to get an automatic nomenclature of the experiments\n",
    "\n",
    "from baselines.get_data import get_full_data\n",
    "# The function get_full_data(dataset, scaling, n_train)\n",
    "# given in baselines.get_data, takes as inputs the name of the dataset (such as 'mnist'),\n",
    "# whether the data is scaled, i.e., scaling=True or False and the number of points n_train\n",
    "# to consider in the dataset. This function returns a dictionnary\n",
    "# full_data = dict(train=dict(X=..., y=...), test=dict(X=..., y=...))\n",
    "# where X are matrices of the data points (stacked by rows) and y are vectors of labels.\n",
    "\n",
    "data_path = os.path.dirname(os.path.abspath(''))\n",
    "data_path = os.path.split(data_path)[0]\n",
    "data_path = os.path.join(data_path, 'experiments', 'baselines', 'datasets')\n",
    "def get_hyper_params_ref(dataset, scaling=True):\n",
    "    \"\"\"\n",
    "    Compute once and for all the reference hyper-parameters\n",
    "    for the dataset such as the bandwidth using the median heuristic\n",
    "    \"\"\"\n",
    "    subsample = 5\n",
    "    dataset_path = os.path.join(data_path, dataset)\n",
    "    data_cfg = dict(scaling=scaling)\n",
    "    file_path = os.path.join(dataset_path, var_to_str(data_cfg) + '_hyper_params_ref.pt')\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        hyperparams_ref = torch.load(open(file_path, 'rb'))\n",
    "    else:\n",
    "        full_data = get_full_data(dataset, scaling=scaling)\n",
    "        X_train = full_data['train']['X']\n",
    "        X_train = X_train[::subsample]\n",
    "        del full_data\n",
    "\n",
    "        cov = (X_train.t().mm(X_train)/len(X_train)).numpy()\n",
    "        lambda_max = np.real(splinalg.eigs(cov, 1)[0])\n",
    "        reg_ref = (lambda_max/np.trace(cov)).item()\n",
    "        reg_ref = float('{:1.2e}'.format(reg_ref))\n",
    "        print(f'reg ref: {reg_ref}')\n",
    "        del cov\n",
    "\n",
    "        dists = torch.cdist(X_train, X_train)**2\n",
    "        del X_train\n",
    "        dists = torch.triu(dists)\n",
    "        sigma_ref = torch.sqrt(torch.median(dists[dists != 0])/2).item()\n",
    "        sigma_ref = float('{:1.2e}'.format(sigma_ref))\n",
    "        print(f'sigma ref: {sigma_ref}')\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        hyperparams_ref = dict(reg_ref=reg_ref, sigma_ref=sigma_ref)\n",
    "        torch.save(hyperparams_ref, open(file_path, 'wb'))\n",
    "    return hyperparams_ref\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the hyper-parameters are fixed, we compute the Gram matrices and save them for further use in ``compute_gram`` and\n",
    "``preprocess_gram``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(X, Y, kernel='linear', reg=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Compute gram matrix between set of points\n",
    "    :param X: matrix of points stacked in rows, i.e., X = (x_1,..., x_n)^T\n",
    "    :param Y: matrix of points stacked in rows, i.e., Y = (y_1,..., y_n)^T\n",
    "    :param kernel: choice of kernel to use\n",
    "    :param sigma: bandwidth for rbf kernel\n",
    "    :param reg: regularization for kernel defined by the inverse of the covariance matrix\n",
    "\n",
    "    :return: Gram matrix of X and Y according to the chosen kernel\n",
    "    \"\"\"\n",
    "    if kernel == 'linear':\n",
    "        gram = X.mm(Y.t())\n",
    "    elif kernel == 'svd':\n",
    "        gram = X.mm(torch.solve(Y.t(), Y.t().mm(Y)/Y.size(0) + reg*torch.eye(Y.size(1)))[0])\n",
    "    elif kernel == 'rbf':\n",
    "        kernel_func = RBF(sigma)\n",
    "        gram = kernel_func(X, Y)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return gram\n",
    "\n",
    "def preprocess_gram(dataset, scaling, n_train=None, kernel='linear', reg=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrices of the train and test points in the dataset\n",
    "    and save it for further use\n",
    "    (e.g., if we run the same experiment but with a different number of labels)\n",
    "    \"\"\"\n",
    "    data_cfg = dict(scaling=scaling, kernel=kernel)\n",
    "    if reg is not None:\n",
    "        data_cfg.update(reg=reg)\n",
    "    if sigma is not None:\n",
    "        data_cfg.update(sigma=sigma)\n",
    "    if n_train is not None:\n",
    "        data_cfg.update(n_train=n_train)\n",
    "    file_path = os.path.join(data_path, dataset, var_to_str(data_cfg) + 'gram_data.pt')\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        gram_data = torch.load(open(file_path, 'rb'))\n",
    "    else:\n",
    "        full_data = get_full_data(dataset, scaling, n_train)\n",
    "        gram_train = compute_gram(full_data['train']['X'], full_data['train']['X'], kernel, reg, sigma)\n",
    "        gram_test = compute_gram(full_data['test']['X'], full_data['train']['X'], kernel, reg, sigma)\n",
    "\n",
    "        gram_data = dict(train=dict(gram=gram_train, y=full_data['train']['y']),\n",
    "                         test=dict(gram=gram_test, y=full_data['test']['y']), n_class=full_data['n_class'])\n",
    "        torch.save(gram_data, open(file_path, 'wb'))\n",
    "    return gram_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the whole pipeline is given in ``get_preprocessed_data`` where part of the labels are\n",
    "removed to simulate a semi-supervised learning setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data(dataset, scaling, n_train, seed, n_labels, kernel, reg=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrices of the data with some reference hyper-parameters\n",
    "    In addition remove some labels to simulate a semi-supervised setting\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    hyper_params_ref = get_hyper_params_ref(dataset, scaling)\n",
    "    if kernel == 'svd' and reg is None:\n",
    "        reg = hyper_params_ref['reg_ref']\n",
    "    if kernel == 'rbf' and sigma is None:\n",
    "        sigma = hyper_params_ref['sigma_ref']\n",
    "\n",
    "    gram_data = preprocess_gram(dataset, scaling, n_train, kernel, reg, sigma)\n",
    "    idx_labeled = keep_labels(gram_data['train']['y'], seed, n_labels)\n",
    "    gram_data.update(idx_labeled=idx_labeled)\n",
    "    return gram_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised k-means\n",
    "The following code implements the semi-supervised k-means as presented in pseudo-code in the introduction\n",
    "(see ``semi_sup_kmeans``). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from baselines.kernels_utils import one_hot_embedding\n",
    "from baselines.semi_sup_kmeans import kmeans_init\n",
    "\n",
    "def semi_sup_kmeans(gram, labels, idx_labeled, n_clust, max_iter=50):\n",
    "    \"\"\"\n",
    "    Implement the semi-supervised k-means algorithm with seeding.\n",
    "    If there are no labels, i.e., len(idx_labeled)==0, then implement a k-means\n",
    "    with k--means++ initialization\n",
    "\n",
    "    Since we do not have access to a finite representation of the centers if we use a kernel,\n",
    "    we directly plug the current solution for the centers given as\n",
    "    C = (ZZ^T)^{-1}Z^T X\n",
    "    The computations follow using some properties of the assignment matrix such as (ZZ^T)^{-1} = diag(Z^T 1)^{-1}\n",
    "    To ensure that the known labels are well assigned, we fix them after each iteration.\n",
    "\n",
    "    :param gram: Gram matrix\n",
    "    :param labels: known labels\n",
    "    :param idx_labeled: Indexes of the known labels\n",
    "    :param n_clust: Number of clusters to form (a priori same as the number of classes)\n",
    "    :param max_iter: Maximum number of iterations of k-means with seeding.\n",
    "    :return: Z: Assignment matrix\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    k = n_clust\n",
    "    n = gram.size(0)\n",
    "    if len(idx_labeled) > 0:\n",
    "        known_labels = one_hot_embedding(labels[idx_labeled], k)\n",
    "        n_lab, k = known_labels.size(0), known_labels.size(1)\n",
    "        Z = torch.zeros(n, k)\n",
    "        Z[idx_labeled] = known_labels\n",
    "    else:\n",
    "        Z = kmeans_init(gram, k)\n",
    "\n",
    "    # Repeat\n",
    "    for i in range(max_iter):\n",
    "        if i % 10 == 5:\n",
    "            norm_equiv = Z.mm(torch.diag(1 / Z.sum(dim=0)).mm(Z.t()))\n",
    "            prev_obj = torch.trace((torch.eye(n) - norm_equiv)*gram)\n",
    "\n",
    "        Z_norm = Z*(1 / Z.sum(dim=0))[:, ]\n",
    "        aux = gram.mm(Z_norm)\n",
    "        dists = -2*aux + torch.ones(n).ger(torch.sum(Z_norm*aux, dim=0))\n",
    "        Z = one_hot_embedding(dists.argmin(dim=1), k)\n",
    "        if len(idx_labeled) > 0:\n",
    "            Z[idx_labeled] = known_labels\n",
    "\n",
    "        if i % 10 == 5:\n",
    "            norm_equiv = Z.mm(torch.diag(1 / Z.sum(dim=0)).mm(Z.t()))\n",
    "            obj = torch.trace((torch.eye(n) - norm_equiv)*gram)\n",
    "            if torch.abs(prev_obj-obj) < 1e-6:\n",
    "                print('kmeans converged in {} iterations'.format(i))\n",
    "                break\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the assignment of the training data has been computed, we can test our clusters\n",
    "on new sets of data points. Note that the clusterings we obtain may not be aligned with the true labels in ``predict``.\n",
    "Imagine that we found the perfect clusters. Their order, i.e., the actual number of the label,\n",
    "may still differ from the true labels. To obtain a fair measure of performance,\n",
    "we align the predictions with the true labels in ``align_pred_test_lab``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(new_gram, gram, Z):\n",
    "    \"\"\"\n",
    "    Given a computed assignment matrix, from which clusters can be computed,\n",
    "    predict the assignments of a new set of points whose Gram matrix\n",
    "    with the training data is given in new_gram\n",
    "\n",
    "    :param new_gram: Gram matrix between test and train data\n",
    "    :param gram: Gram matrix of the train data\n",
    "    :param Z: assignment matrix of the training data\n",
    "    :return: y_pred predicted assignments for the new points\n",
    "    whose Gram matrix with the train data was given in new_gram\n",
    "    \"\"\"\n",
    "    Z_norm = Z * (1 / Z.sum(dim=0))[:, ]\n",
    "    aux = gram.mm(Z_norm)\n",
    "    norm_centers = torch.sum(Z_norm * aux, dim=0)\n",
    "\n",
    "    n_new = new_gram.size(0)\n",
    "    Z_norm = Z * (1 / Z.sum(dim=0))[:, ]\n",
    "    dists = -2*new_gram.mm(Z_norm) + torch.ones(n_new).ger(norm_centers)\n",
    "    y_pred = dists.argmin(dim=1)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def align_pred_test_lab(y_pred, y_test, k):\n",
    "    \"\"\"\n",
    "    The algorithm can assign new points to given clusters\n",
    "    but the actual labels may differ, since the ordering of the clusters\n",
    "    may not taken into account if there are no labels for example.\n",
    "    This function realigns the numbering of the clusters such that\n",
    "    it finds the most relevant numbering for the predictions\n",
    "\n",
    "    :param y_pred: predicted labels\n",
    "    :param y_test: true labels\n",
    "    :param k: number of clusters\n",
    "\n",
    "    :return: y_pred_aligned: predicted labels whose numbering\n",
    "    has been reassigned to match the true numbering in the best possible way\n",
    "    \"\"\"\n",
    "    # Relabel classes\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-1 * conf_matrix)\n",
    "\n",
    "    y_pred_aligned = -1 * np.ones_like(y_pred)\n",
    "    for i in range(k):\n",
    "        idxs = np.where(y_pred == i)[0]\n",
    "        label = np.where(col_ind == i)[0][0]\n",
    "        y_pred_aligned[idxs] = int(label)\n",
    "    y_pred_aligned = torch.from_numpy(y_pred_aligned.flatten())\n",
    "    return y_pred_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole algorithm is then presented in ``predict_with_semi_sup_kmeans``. It takes as inputs the Gram matrices of the data preprocessed in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_semi_sup_kmeans(data, max_iter=50):\n",
    "    \"\"\"\n",
    "    Run semi-supervised k-means on the training data, then test it on new points\n",
    "\n",
    "    :param: data as output from the function get_preprocessed_data\n",
    "    :param: max_iter number of iterations of kmeans\n",
    "    :return: accuracy on test data\n",
    "    \"\"\"\n",
    "    # Algo\n",
    "    gram_train, gram_test = data['train']['gram'], data['test']['gram']\n",
    "    y_train, y_test = data['train']['y'], data['test']['y']\n",
    "    idx_labeled, n_class = data['idx_labeled'], data['n_class']\n",
    "    Z = semi_sup_kmeans(gram_train, y_train, idx_labeled, n_class, max_iter)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = predict(gram_test, gram_train, Z)\n",
    "    y_pred = align_pred_test_lab(y_pred, y_test, n_class)\n",
    "    acc_test = torch.sum(y_pred == y_test).float() / len(y_pred)\n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "We can then combine the preprocessing of the data with the actual algorithm and test our baseline on some dataset.\n",
    "The following functions are simple wrappers that avoid recomputing the experiments twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "def exp_core(dataset, scaling=True, n_train=None, seed=0, n_labels=0,\n",
    "             kernel='linear', sigma=None, reg=None, max_iter=100):\n",
    "    data = get_preprocessed_data(dataset, scaling, n_train, seed, n_labels, kernel, reg, sigma)\n",
    "    test_acc = predict_with_semi_sup_kmeans(data, max_iter)\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "def wrapped_exp_core(data_cfg, optim_cfg):\n",
    "    exp_cfg = dict(data_cfg=data_cfg, optim_cfg=optim_cfg)\n",
    "    print(*['{0}:{1}'.format(key, value) for key, value in exp_cfg.items()], sep='\\n')\n",
    "    exp_path = os.path.dirname(os.path.abspath(''))\n",
    "    exp_path = os.path.join(exp_path, 'experiments', 'baselines')\n",
    "    file_path = os.path.join(exp_path, 'results', var_to_str(data_cfg), var_to_str(optim_cfg) + '.pt')\n",
    "    if os.path.exists(file_path):\n",
    "        _, test_acc = torch.load(open(file_path, 'rb'))\n",
    "    else:\n",
    "        test_acc = exp_core(**data_cfg, **optim_cfg)\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        torch.save([exp_cfg, test_acc], open(file_path, 'wb'))\n",
    "\n",
    "    result = {**data_cfg, **optim_cfg}\n",
    "    result.update(test_acc=test_acc.item())\n",
    "    result = DataFrame(result, index=[0])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present an example on a subset of the MNIST dataset below. Full comparisons are provided in the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from baselines.pipeline_utils import build_list_exp\n",
    "from baselines.plot_tools import plot_exp\n",
    "dataset = 'mnist'\n",
    "scaling = True\n",
    "kernel = 'linear'\n",
    "data_cfg = dict(dataset=dataset, seed=0, scaling=scaling,\n",
    "                n_labels=[50 * i for i in range(0, 5)], kernel=kernel)\n",
    "data_cfg.update(n_train=500)\n",
    "optim_cfg = dict(max_iter=100)\n",
    "exp_cfgs = build_list_exp([dict(data_cfg=data_cfg, optim_cfg=optim_cfg)])\n",
    "\n",
    "results = DataFrame()\n",
    "for exp_cfg in exp_cfgs:\n",
    "    result = wrapped_exp_core(exp_cfg['data_cfg'], exp_cfg['optim_cfg'])\n",
    "    results = results.append(result, ignore_index=True)\n",
    "\n",
    "plot_exp(results, max_n_labels=200, add_xsdc_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[1] Basu S., Banerjee A., Mooney R. (2002) \"Semi-supervised clustering by seeding\".\n",
    "*International Conference on Machine Learning.*  \n",
    "[2] Jones, C., Roulet, V., Harchaoui, Z. (2022) \"Discriminative Clustering\n",
    "with Representation Learning with any Ratio of Labeled to Unlabeled Data\".\n",
    "*Statistics and Computing.*  \n",
    "[3] LeCun Y., Cortes C. (2010) \"MNIST handwritten digit database\" http://yann.lecun.com/exdb/mnist/  \n",
    "[4] Fukumizu K., Gretton A., Lanckriet G., Scholkopf B., Sriperumbudur B. K. (2009)\n",
    "\"Kernel choice and classifiability for RKHS embeddings of probability distributions\".\n",
    "*Advances in Neural Information Processing Systems*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
